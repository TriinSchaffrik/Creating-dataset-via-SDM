{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNu1T9ugO6tqd22jPdnd9oR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["#Failide lugemiseks Colabis\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","os.chdir(\"/content/drive/My Drive/AI/semantic-diffusion-model-main\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWABHaYdoyWH","executionInfo":{"status":"ok","timestamp":1681828551232,"user_tz":-180,"elapsed":22354,"user":{"displayName":"Triin Schaffrik","userId":"05263587151989347851"}},"outputId":"a83ab364-bcf9-4e28-e83a-f04ef8fe2f3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install lpips"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKqXjHPsu0jA","executionInfo":{"status":"ok","timestamp":1681828555079,"user_tz":-180,"elapsed":3891,"user":{"displayName":"Triin Schaffrik","userId":"05263587151989347851"}},"outputId":"51eee440-029c-45a4-ea9a-9e64ffc483fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (4.65.0)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.10.1)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from lpips) (0.15.1+cu118)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from lpips) (2.0.0+cu118)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from lpips) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.11.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips) (1.11.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (16.0.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=0.4.0->lpips) (3.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (8.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips) (2.27.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.12.7)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n","Installing collected packages: lpips\n","Successfully installed lpips-0.1.4\n"]}]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":713},"id":"1fUcNy2ZoXnP","executionInfo":{"status":"error","timestamp":1681830517687,"user_tz":-180,"elapsed":396,"user":{"displayName":"Triin Schaffrik","userId":"05263587151989347851"}},"outputId":"cbb3c714-a0c5-4012-d754-20df63b77ac4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating LPIPS given root path RESULTS/2023-04-14\n","Preparing DataLoader for the evaluation phase...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["['RESULTS/2023-04-14/samples'] 5\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-cabd9eb0ec95>\u001b[0m in \u001b[0;36m<cell line: 129>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'batch size to use'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mlpips_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_lpips_given_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LPIPS: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlpips_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-cabd9eb0ec95>\u001b[0m in \u001b[0;36mcalculate_lpips_given_paths\u001b[0;34m(root_path, img_size, batch_size, test_list)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mgroup_of_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaders_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mlpips_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_lpips_given_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_of_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-cabd9eb0ec95>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_clips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mgroup_of_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaders_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0mlpips_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_lpips_given_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_of_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: '_MultiProcessingDataLoaderIter' object has no attribute 'next'"]}],"source":["from traitlets.traitlets import default\n","\"\"\"\n","StarGAN v2\n","Copyright (c) 2020-present NAVER Corp.\n","\n","This work is licensed under the Creative Commons Attribution-NonCommercial\n","4.0 International License. To view a copy of this license, visit\n","http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n","Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n","\"\"\"\n","\n","import os\n","import torch\n","import argparse\n","import torch.nn as nn\n","from torchvision import models\n","from evaluations.default_dataset import get_eval_loader\n","\n","\n","def normalize(x, eps=1e-10):\n","    return x * torch.rsqrt(torch.sum(x**2, dim=1, keepdim=True) + eps)\n","\n","\n","class AlexNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layers = models.alexnet(pretrained=True).features\n","        self.channels = []\n","        for layer in self.layers:\n","            if isinstance(layer, nn.Conv2d):\n","                self.channels.append(layer.out_channels)\n","\n","    def forward(self, x):\n","        fmaps = []\n","        for layer in self.layers:\n","            x = layer(x)\n","            if isinstance(layer, nn.ReLU):\n","                fmaps.append(x)\n","        return fmaps\n","\n","\n","class Conv1x1(nn.Module):\n","    def __init__(self, in_channels, out_channels=1):\n","        super().__init__()\n","        self.main = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False))\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","class LPIPS(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.alexnet = AlexNet()\n","        self.lpips_weights = nn.ModuleList()\n","        for channels in self.alexnet.channels:\n","            self.lpips_weights.append(Conv1x1(channels, 1))\n","        self._load_lpips_weights()\n","        # imagenet normalization for range [-1, 1]\n","        self.mu = torch.tensor([-0.03, -0.088, -0.188]).view(1, 3, 1, 1).cuda()\n","        self.sigma = torch.tensor([0.458, 0.448, 0.450]).view(1, 3, 1, 1).cuda()\n","\n","    def _load_lpips_weights(self):\n","        own_state_dict = self.state_dict()\n","        if torch.cuda.is_available():\n","            state_dict = torch.load('metrics/lpips_weights.ckpt')\n","        else:\n","            state_dict = torch.load('metrics/lpips_weights.ckpt',\n","                                    map_location=torch.device('cpu'))\n","        for name, param in state_dict.items():\n","            if name in own_state_dict:\n","                own_state_dict[name].copy_(param)\n","\n","    def forward(self, x, y):\n","        x = (x - self.mu) / self.sigma\n","        y = (y - self.mu) / self.sigma\n","        x_fmaps = self.alexnet(x)\n","        y_fmaps = self.alexnet(y)\n","        lpips_value = 0\n","        for x_fmap, y_fmap, conv1x1 in zip(x_fmaps, y_fmaps, self.lpips_weights):\n","            x_fmap = normalize(x_fmap)\n","            y_fmap = normalize(y_fmap)\n","            lpips_value += torch.mean(conv1x1((x_fmap - y_fmap)**2))\n","        return lpips_value\n","\n","\n","@torch.no_grad()\n","def calculate_lpips_given_images(group_of_images):\n","    # group_of_images = [torch.randn(N, C, H, W) for _ in range(10)]\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    lpips = LPIPS().eval().to(device)\n","    lpips_values = []\n","    num_rand_outputs = len(group_of_images)\n","\n","    # calculate the average of pairwise distances among all random outputs\n","    for i in range(num_rand_outputs-1):\n","        for j in range(i+1, num_rand_outputs):\n","            lpips_values.append(lpips(group_of_images[i], group_of_images[j]))\n","    lpips_value = torch.mean(torch.stack(lpips_values, dim=0))\n","    return lpips_value.item()\n","\n","\n","@torch.no_grad()\n","def calculate_lpips_given_paths(root_path, img_size=256, batch_size=50, test_list=None):\n","    print('Calculating LPIPS given root path %s' % root_path)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    paths = [os.path.join(root_path, path) for path in os.listdir(root_path) if path.startswith('samples')]\n","    loaders = [get_eval_loader(path, img_size, batch_size, shuffle=False, drop_last=False, test_list=test_list) for path in paths]\n","    loaders_iter = [iter(loader) for loader in loaders]\n","    num_clips = len(loaders[0])\n","    print(paths, num_clips)\n","\n","    lpips_values = []\n","\n","    for i in range(num_clips):\n","        group_of_images = [loader.next().cuda() for loader in loaders_iter]\n","        lpips_values.append(calculate_lpips_given_images(group_of_images))\n","\n","    lpips_values = torch.tensor(lpips_values)\n","    print(lpips_values)\n","\n","    lpips_mean = torch.mean(lpips_values)\n","\n","    return lpips_mean\n","\n","if __name__ == '__main__':\n","    # python -m metrics.lpips --paths PATH_REAL PATH_FAKE\n","    \n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-f')\n","    parser.add_argument('--paths', type=str, default=\"images samples\")\n","    parser.add_argument('--root_path', type=str, default=\"RESULTS/2023-04-14\", help='paths to real and fake images')\n","    parser.add_argument('--img_size', type=int, default=256, help='image resolution')\n","    parser.add_argument('--batch_size', type=int, default=50, help='batch size to use')\n","    args = parser.parse_args()\n","    lpips_value = calculate_lpips_given_paths(args.root_path, args.img_size, args.batch_size)\n","    print('LPIPS: ', lpips_value)"]},{"cell_type":"code","source":["# calculate inception score with Keras\n","from math import floor\n","from numpy import ones\n","from numpy import expand_dims\n","from numpy import log\n","from numpy import mean\n","from numpy import std\n","from numpy import exp\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n"," \n","# assumes images have the shape 299x299x3, pixels in [0,255]\n","def calculate_inception_score(images, n_split=10, eps=1E-16):\n"," # load inception v3 model\n"," model = InceptionV3()\n"," # convert from uint8 to float32\n"," processed = images.astype('float32')\n"," # pre-process raw images for inception v3 model\n"," processed = preprocess_input(processed)\n"," # predict class probabilities for images\n"," yhat = model.predict(processed)\n"," # enumerate splits of images/predictions\n"," scores = list()\n"," n_part = floor(images.shape[0] / n_split)\n"," for i in range(n_split):\n","  # retrieve p(y|x)\n","  ix_start, ix_end = i * n_part, i * n_part + n_part\n","  p_yx = yhat[ix_start:ix_end]\n","  # calculate p(y)\n","  p_y = expand_dims(p_yx.mean(axis=0), 0)\n","  # calculate KL divergence using log probabilities\n","  kl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n","  # sum over classes\n","  sum_kl_d = kl_d.sum(axis=1)\n","  # average over images\n","  avg_kl_d = mean(sum_kl_d)\n","  # undo the log\n","  is_score = exp(avg_kl_d)\n","  # store\n","  scores.append(is_score)\n"," # average across images\n"," is_avg, is_std = mean(scores), std(scores)\n"," return is_avg, is_std\n"," \n","# pretend to load images\n","print('loaded', images.shape)\n","# calculate inception score\n","#is_avg, is_std = calculate_inception_score(images)\n","print('score', is_avg, is_std)"],"metadata":{"id":"YBKDCzsbx4N6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681895072329,"user_tz":-180,"elapsed":4,"user":{"displayName":"Triin Schaffrik","userId":"05263587151989347851"}},"outputId":"e864f2c2-a805-4c65-b1a9-0fec0fbee936"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n","loaded (50, 299, 299, 3)\n","score 1.0 0.0\n"]}]},{"cell_type":"code","source":["import torch\n","_ = torch.manual_seed(123)\n","from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n","lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg')\n","# LPIPS needs the images to be in the [-1, 1] range.\n","img1 = (torch.rand(10, 3, 100, 100) * 2) - 1\n","img2 = (torch.rand(10, 3, 100, 100) * 2) - 1\n","lpips(img1, img2)"],"metadata":{"id":"mZqbIFMowh59"},"execution_count":null,"outputs":[]}]}